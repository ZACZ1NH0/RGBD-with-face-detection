{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49c3b1db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T09:37:41.273554Z",
     "iopub.status.busy": "2026-01-01T09:37:41.273182Z",
     "iopub.status.idle": "2026-01-01T09:37:49.812081Z",
     "shell.execute_reply": "2026-01-01T09:37:49.811471Z"
    },
    "papermill": {
     "duration": 8.544129,
     "end_time": "2026-01-01T09:37:49.813435",
     "exception": false,
     "start_time": "2026-01-01T09:37:41.269306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "\n",
    "# ==========================================\n",
    "# 1. DATASET: S·ª¨A L·∫†I DEPTH TH√ÄNH 1 K√äNH (GRAYSCALE)\n",
    "# ==========================================\n",
    "\n",
    "class RGBDTripletDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.rgb_color_aug = transforms.ColorJitter(brightness=0.2, contrast=0.2)\n",
    "        \n",
    "        # Normalize\n",
    "        self.norm_rgb = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.norm_depth = transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "        self.people_data = {}\n",
    "        self.people_names = []\n",
    "        \n",
    "        # C·∫•u h√¨nh t√™n folder ch√≠nh x√°c c·ªßa b·∫°n\n",
    "        RGB_FOLDER_NAME = \"dataset_face\"   # Folder ch·ª©a ·∫£nh m√†u\n",
    "        DEPTH_FOLDER_NAME = \"dataset_depth\" # Folder ch·ª©a ·∫£nh depth\n",
    "        \n",
    "        print(f\"üîÑ ƒêang qu√©t d·ªØ li·ªáu... (T√¨m folder '{RGB_FOLDER_NAME}' v√† '{DEPTH_FOLDER_NAME}')\")\n",
    "        \n",
    "        if not os.path.exists(root_dir):\n",
    "             print(f\"‚ùå Error: Kh√¥ng t√¨m th·∫•y ƒë∆∞·ªùng d·∫´n g·ªëc {root_dir}\")\n",
    "             return\n",
    "\n",
    "        # Duy·ªát qua t·ª´ng ng∆∞·ªùi (v√≠ d·ª•: Longvu)\n",
    "        for person_name in os.listdir(root_dir):\n",
    "            person_path = os.path.join(root_dir, person_name)\n",
    "            if not os.path.isdir(person_path): continue\n",
    "            \n",
    "            # T·∫°o ƒë∆∞·ªùng d·∫´n t·ªõi folder con\n",
    "            rgb_dir = os.path.join(person_path, RGB_FOLDER_NAME)\n",
    "            depth_dir = os.path.join(person_path, DEPTH_FOLDER_NAME)\n",
    "            \n",
    "            # Ki·ªÉm tra c·∫£ 2 folder c√≥ t·ªìn t·∫°i kh√¥ng\n",
    "            if os.path.isdir(rgb_dir) and os.path.isdir(depth_dir):\n",
    "                \n",
    "                # 1. L·∫•y danh s√°ch file Depth tr∆∞·ªõc (ƒë·ªÉ l√†m map tra c·ª©u)\n",
    "                # Map: {'face_0': 'ƒë∆∞·ªùng_d·∫´n_full/face_0.png'}\n",
    "                depth_files_map = {}\n",
    "                for f in os.listdir(depth_dir):\n",
    "                    if f.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                        name_no_ext = os.path.splitext(f)[0] # L·∫•y t√™n \"face_0\" b·ªè ƒëu√¥i\n",
    "                        depth_files_map[name_no_ext] = os.path.join(depth_dir, f)\n",
    "                \n",
    "                paired_rgb = []\n",
    "                \n",
    "                # 2. Duy·ªát file RGB v√† t√¨m c·∫∑p trong Depth\n",
    "                rgb_files = [f for f in os.listdir(rgb_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "                \n",
    "                for f in rgb_files:\n",
    "                    name_no_ext = os.path.splitext(f)[0] # \"face_0\"\n",
    "                    \n",
    "                    # N·∫øu t√™n n√†y c√≥ trong depth map -> C√≥ c·∫∑p!\n",
    "                    if name_no_ext in depth_files_map:\n",
    "                        rgb_full_path = os.path.join(rgb_dir, f)\n",
    "                        depth_full_path = depth_files_map[name_no_ext]\n",
    "                        paired_rgb.append((rgb_full_path, depth_full_path))\n",
    "                \n",
    "                # N·∫øu ng∆∞·ªùi n√†y c√≥ √≠t nh·∫•t 2 c·∫∑p ·∫£nh (ƒë·ªÉ ch·ªçn pos/neg)\n",
    "                if len(paired_rgb) > 1:\n",
    "                    self.people_data[person_name] = paired_rgb\n",
    "                    self.people_names.append(person_name)\n",
    "        \n",
    "        print(f\"‚úÖ ƒê√£ load: {len(self.people_names)} ng∆∞·ªùi.\")\n",
    "        if len(self.people_names) == 0:\n",
    "            print(\"‚ùå V·∫´n ch∆∞a t√¨m th·∫•y ·∫£nh! H√£y ki·ªÉm tra k·ªπ t√™n folder con b√™n trong.\")\n",
    "            print(f\"   V√≠ d·ª• mong ƒë·ª£i: {root_dir}/Longvu/{RGB_FOLDER_NAME}/face_0.jpg\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum([len(imgs) for imgs in self.people_data.values()])\n",
    "\n",
    "    def load_tuple(self, pair_paths):\n",
    "        rgb_path, depth_path = pair_paths\n",
    "        \n",
    "        # 1. Load ·∫£nh (Ch·ªâ m·ªü file, CH∆ØA transform th√†nh tensor)\n",
    "        try:\n",
    "            rgb_pil = Image.open(rgb_path).convert(\"RGB\")\n",
    "            depth_pil = Image.open(depth_path).convert(\"L\") # Grayscale\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå L·ªói: {rgb_path}\")\n",
    "            # Tr·∫£ v·ªÅ ·∫£nh ƒëen n·∫øu l·ªói\n",
    "            rgb_pil = Image.new('RGB', (224, 224))\n",
    "            depth_pil = Image.new('L', (224, 224))\n",
    "\n",
    "        # Tr·∫£ v·ªÅ ·∫£nh PIL nguy√™n b·∫£n ƒë·ªÉ ƒë∆∞a v√†o sync\n",
    "        return rgb_pil, depth_pil\n",
    "\n",
    "    def transform_sync(self, rgb_img, depth_img):\n",
    "        \"\"\"H√†m bi·∫øn ƒë·ªïi ƒë·ªìng b·ªô: RGB l·∫≠t th√¨ Depth c≈©ng l·∫≠t\"\"\"\n",
    "        \n",
    "        # 1. Resize (B·∫Øt bu·ªôc gi·ªëng nhau)\n",
    "        rgb_img = TF.resize(rgb_img, (224, 224))\n",
    "        depth_img = TF.resize(depth_img, (224, 224))\n",
    "        \n",
    "        # 2. Random Horizontal Flip (ƒê·ªíNG B·ªò)\n",
    "        if random.random() > 0.5:\n",
    "            rgb_img = TF.hflip(rgb_img)\n",
    "            depth_img = TF.hflip(depth_img)\n",
    "            \n",
    "        # 3. Random Rotation (ƒê·ªíNG B·ªò - Quan tr·ªçng v√¨ d·ªØ li·ªáu √≠t)\n",
    "        if random.random() > 0.5:\n",
    "            angle = random.uniform(-10, 10)\n",
    "            rgb_img = TF.rotate(rgb_img, angle)\n",
    "            depth_img = TF.rotate(depth_img, angle)\n",
    "            \n",
    "        # 4. Color Jitter (CH·ªà RGB - Kh√¥ng l√†m bi·∫øn ƒë·ªïi h√¨nh h·ªçc)\n",
    "        # Depth kh√¥ng ƒë∆∞·ª£c ch·ªânh m√†u v√¨ s·∫Ω sai gi√° tr·ªã ƒë·ªô s√¢u\n",
    "        rgb_img = self.rgb_color_aug(rgb_img)\n",
    "        \n",
    "        # 5. ToTensor & Normalize (Ri√™ng bi·ªát)\n",
    "        rgb_t = TF.to_tensor(rgb_img)\n",
    "        rgb_t = self.norm_rgb(rgb_t)\n",
    "        \n",
    "        depth_t = TF.to_tensor(depth_img)\n",
    "        depth_t = self.norm_depth(depth_t)\n",
    "        \n",
    "        return rgb_t, depth_t\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Ch·ªçn Anchor\n",
    "        anchor_name = random.choice(self.people_names)\n",
    "        anchor_imgs = self.people_data[anchor_name]\n",
    "        anchor_pair = random.choice(anchor_imgs)\n",
    "        \n",
    "        # Ch·ªçn Positive\n",
    "        pos_pair = random.choice(anchor_imgs)\n",
    "        while pos_pair == anchor_pair and len(anchor_imgs) > 1:\n",
    "            pos_pair = random.choice(anchor_imgs)\n",
    "            \n",
    "        # Ch·ªçn Negative\n",
    "        neg_name = random.choice(self.people_names)\n",
    "        while neg_name == anchor_name:\n",
    "            neg_name = random.choice(self.people_names)\n",
    "        neg_pair = random.choice(self.people_data[neg_name])\n",
    "        \n",
    "        a_rgb_pil, a_dep_pil = self.load_tuple(anchor_pair)\n",
    "        p_rgb_pil, p_dep_pil = self.load_tuple(pos_pair)\n",
    "        n_rgb_pil, n_dep_pil = self.load_tuple(neg_pair)\n",
    "\n",
    "        # Transform ƒë·ªìng b·ªô\n",
    "        a_rgb, a_d = self.transform_sync(a_rgb_pil, a_dep_pil)\n",
    "        p_rgb, p_d = self.transform_sync(p_rgb_pil, p_dep_pil)\n",
    "        n_rgb, n_d = self.transform_sync(n_rgb_pil, n_dep_pil)\n",
    "\n",
    "        return {\n",
    "            \"anchor\": (a_rgb, a_d),\n",
    "            \"positive\": (p_rgb, p_d),\n",
    "            \"negative\": (n_rgb, n_d)\n",
    "        }\n",
    "\n",
    "# ==========================================\n",
    "# 2. MODEL: X·ª¨ L√ù WEIGHTS TH√îNG MINH\n",
    "# ==========================================\n",
    "\n",
    "def smart_load_weights(model, weight_path, device):\n",
    "    \"\"\"H√†m h·ªó tr·ª£ load weights b·ªè qua ti·ªÅn t·ªë 'backbone.' ho·∫∑c 'module.' n·∫øu c√≥\"\"\"\n",
    "    state_dict = torch.load(weight_path, map_location=device)\n",
    "    new_state_dict = {}\n",
    "    \n",
    "    for key, value in state_dict.items():\n",
    "        # X√≥a ti·ªÅn t·ªë th∆∞·ªùng g·∫∑p khi train SSL\n",
    "        new_key = key.replace(\"backbone.\", \"\").replace(\"module.\", \"\").replace(\"encoder.\", \"\")\n",
    "        new_state_dict[new_key] = value\n",
    "        \n",
    "    # Load v·ªõi strict=False ƒë·ªÉ b·ªè qua c√°c l·ªõp FC (classifier) b·ªã l·ªách\n",
    "    missing, unexpected = model.load_state_dict(new_state_dict, strict=False)\n",
    "    print(f\"üîπ Load {weight_path}: Missing keys (FC layers): {len(missing)}\")\n",
    "\n",
    "class FaceModelTrain(nn.Module):\n",
    "    def __init__(self, rgb_pth, depth_pth, device, freeze_backbone=False):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # ==========================================\n",
    "        # 1. RGB ENCODER: S·ª¨A TH√ÄNH RESNET18\n",
    "        # ==========================================\n",
    "        print(f\"üîπ ƒêang load RGB Encoder: ResNet18...\")\n",
    "        base_rgb = models.resnet18(weights=None) # <--- S·ª¨A T·ª™ 50 V·ªÄ 18\n",
    "        \n",
    "        # Load weights\n",
    "        smart_load_weights(base_rgb, rgb_pth, device)\n",
    "        \n",
    "        self.rgb_backbone = nn.Sequential(*list(base_rgb.children())[:-1]) \n",
    "        # ResNet18 c√≥ output feature l√† 512 (thay v√¨ 2048 nh∆∞ ResNet50)\n",
    "        self.rgb_projector = nn.Linear(512, 512) \n",
    "\n",
    "        # ==========================================\n",
    "        # 2. DEPTH ENCODER: EFFICIENTNET-B0\n",
    "        # ==========================================\n",
    "        print(f\"üîπ ƒêang load Depth Encoder: EfficientNet-B0...\")\n",
    "        base_depth = models.efficientnet_b0(weights=None)\n",
    "        \n",
    "        # S·ª≠a input conv th√†nh 1 k√™nh (Grayscale)\n",
    "        base_depth.features[0][0] = nn.Conv2d(\n",
    "            1, 32, kernel_size=3, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        \n",
    "        # Load weights\n",
    "        smart_load_weights(base_depth, depth_pth, device)\n",
    "        \n",
    "        self.depth_features = base_depth.features\n",
    "        self.depth_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.depth_projector = nn.Linear(1280, 512)\n",
    "\n",
    "        # ==========================================\n",
    "        # 3. FUSION HEAD\n",
    "        # ==========================================\n",
    "        # Input: 512 (RGB) + 512 (Depth) = 1024\n",
    "        self.fusion_head = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512) \n",
    "        )\n",
    "\n",
    "        # Freeze Backbone logic\n",
    "        if freeze_backbone:\n",
    "            for param in self.rgb_backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.depth_features.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"‚ùÑÔ∏è ƒê√£ ƒë√≥ng bƒÉng Backbone (ResNet18 & EfficientNet).\")\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward_one(self, rgb, depth):\n",
    "        # RGB Stream\n",
    "        x_rgb = self.rgb_backbone(rgb).view(rgb.size(0), -1) # (B, 512)\n",
    "        x_rgb = self.rgb_projector(x_rgb)\n",
    "        x_rgb = F.normalize(x_rgb)\n",
    "\n",
    "        # Depth Stream\n",
    "        x_d = self.depth_features(depth)\n",
    "        x_d = self.depth_pool(x_d).flatten(1) # (B, 1280)\n",
    "        x_d = self.depth_projector(x_d)       # (B, 512)\n",
    "        x_d = F.normalize(x_d)\n",
    "\n",
    "        # Fusion\n",
    "        concat = torch.cat([x_rgb, x_d], dim=1) # (B, 1024)\n",
    "        embedding = self.fusion_head(concat)    # (B, 512)\n",
    "        \n",
    "        return F.normalize(embedding)\n",
    "\n",
    "    def forward(self, a_r, a_d, p_r, p_d, n_r, n_d):\n",
    "        emb_a = self.forward_one(a_r, a_d)\n",
    "        emb_p = self.forward_one(p_r, p_d)\n",
    "        emb_n = self.forward_one(n_r, n_d)\n",
    "        return emb_a, emb_p, emb_n\n",
    "\n",
    "# --- Triplet Loss (Gi·ªØ nguy√™n) ---\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        dist_pos = (anchor - positive).pow(2).sum(1)\n",
    "        dist_neg = (anchor - negative).pow(2).sum(1)\n",
    "        losses = F.relu(dist_pos - dist_neg + self.margin)\n",
    "        return losses.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04c24bd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T09:37:49.818252Z",
     "iopub.status.busy": "2026-01-01T09:37:49.817971Z",
     "iopub.status.idle": "2026-01-01T09:37:49.828099Z",
     "shell.execute_reply": "2026-01-01T09:37:49.827409Z"
    },
    "papermill": {
     "duration": 0.013636,
     "end_time": "2026-01-01T09:37:49.829145",
     "exception": false,
     "start_time": "2026-01-01T09:37:49.815509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # 1. Config\n",
    "    BATCH_SIZE = 32      \n",
    "    LR = 0.0001\n",
    "    EPOCHS = 20\n",
    "    DATA_PATH = \"/kaggle/input/people/lfw_processed\" \n",
    "    RGB_PTH = \"/kaggle/input/deeplearn/rgb_encoder_epoch20.pth\" \n",
    "    DEPTH_PTH = \"/kaggle/input/deeplearn-eff/depth_encoder_epoch20.pth\"\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Device: {DEVICE}\")\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "\n",
    "    # 3. Data Loader\n",
    "    dataset = RGBDTripletDataset(DATA_PATH)\n",
    "    # L∆∞u √Ω: drop_last=True ƒë·ªÉ tr√°nh l·ªói batch l·∫ª 1 m·∫´u g√¢y l·ªói BatchNorm\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True)\n",
    "\n",
    "    # 4. Init Model (ResNet18 + EfficientNet)\n",
    "    model = FaceModelTrain(\n",
    "        rgb_pth=RGB_PTH, \n",
    "        depth_pth=DEPTH_PTH, \n",
    "        device=DEVICE,\n",
    "        freeze_backbone=True \n",
    "    )\n",
    "    model.train()\n",
    "\n",
    "    criterion = TripletLoss(margin=1.0)\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LR)\n",
    "\n",
    "    # 5. Training Loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "        total_correct = 0 # ƒê·∫øm s·ªë m·∫´u ƒë√∫ng\n",
    "        total_samples = 0\n",
    "        \n",
    "        # Unfreeze sau 5 epoch\n",
    "        if epoch == 5:\n",
    "            print(\"üîì Unfreezing Backbones...\")\n",
    "            for param in model.rgb_backbone.parameters(): param.requires_grad = True\n",
    "            for param in model.depth_features.parameters(): param.requires_grad = True\n",
    "            optimizer = optim.Adam(model.parameters(), lr=LR * 0.1)\n",
    "\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            a_r, a_d = batch[\"anchor\"][0].to(DEVICE), batch[\"anchor\"][1].to(DEVICE)\n",
    "            p_r, p_d = batch[\"positive\"][0].to(DEVICE), batch[\"positive\"][1].to(DEVICE)\n",
    "            n_r, n_d = batch[\"negative\"][0].to(DEVICE), batch[\"negative\"][1].to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward\n",
    "            emb_a, emb_p, emb_n = model(a_r, a_d, p_r, p_d, n_r, n_d)\n",
    "\n",
    "            # T√≠nh Loss\n",
    "            loss = criterion(emb_a, emb_p, emb_n)\n",
    "            \n",
    "            # Backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # --- T√çNH ACCURACY ---\n",
    "            with torch.no_grad():\n",
    "                # Kho·∫£ng c√°ch A-P v√† A-N\n",
    "                dist_pos = (emb_a - emb_p).pow(2).sum(1)\n",
    "                dist_neg = (emb_a - emb_n).pow(2).sum(1)\n",
    "                \n",
    "                # ƒê√∫ng n·∫øu dist_pos < dist_neg\n",
    "                pred_correct = (dist_pos < dist_neg).sum().item()\n",
    "                \n",
    "                total_correct += pred_correct\n",
    "                total_samples += a_r.size(0)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # In log chi ti·∫øt\n",
    "            if batch_idx % 5 == 0:\n",
    "                acc_batch = pred_correct / a_r.size(0)\n",
    "                print(f\"Ep {epoch+1} | Batch {batch_idx} | Loss: {loss.item():.4f} | Acc: {acc_batch:.2%}\")\n",
    "\n",
    "        # T·ªïng k·∫øt epoch\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        avg_acc = total_correct / total_samples if total_samples > 0 else 0\n",
    "        print(f\"===> End Ep {epoch+1} | Avg Loss: {avg_loss:.4f} | Avg Acc: {avg_acc:.2%}\")\n",
    "        train_acc.append(avg_acc)\n",
    "        train_loss.append(avg_loss)\n",
    "        \n",
    "        if (epoch+1) % 5 == 0:\n",
    "            torch.save(model.state_dict(), f\"fusion_face_ep{epoch+1}.pth\")\n",
    "\n",
    "    np.save(\"train_loss.npy\", np.array(train_loss))\n",
    "    np.save(\"train_accuracy.npy\", np.array(train_acc))\n",
    "    torch.save(model.state_dict(), \"fusion_face_final.pth\")\n",
    "    print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0455e2d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T09:37:49.832915Z",
     "iopub.status.busy": "2026-01-01T09:37:49.832718Z",
     "iopub.status.idle": "2026-01-01T10:13:26.278072Z",
     "shell.execute_reply": "2026-01-01T10:13:26.277297Z"
    },
    "papermill": {
     "duration": 2136.449155,
     "end_time": "2026-01-01T10:13:26.279796",
     "exception": false,
     "start_time": "2026-01-01T09:37:49.830641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "üîÑ ƒêang qu√©t d·ªØ li·ªáu... (T√¨m folder 'dataset_face' v√† 'dataset_depth')\n",
      "‚úÖ ƒê√£ load: 423 ng∆∞·ªùi.\n",
      "üîπ ƒêang load RGB Encoder: ResNet18...\n",
      "üîπ Load /kaggle/input/deeplearn/rgb_encoder_epoch20.pth: Missing keys (FC layers): 2\n",
      "üîπ ƒêang load Depth Encoder: EfficientNet-B0...\n",
      "üîπ Load /kaggle/input/deeplearn-eff/depth_encoder_epoch20.pth: Missing keys (FC layers): 2\n",
      "‚ùÑÔ∏è ƒê√£ ƒë√≥ng bƒÉng Backbone (ResNet18 & EfficientNet).\n",
      "Ep 1 | Batch 0 | Loss: 0.9039 | Acc: 71.88%\n",
      "Ep 1 | Batch 5 | Loss: 0.8927 | Acc: 68.75%\n",
      "Ep 1 | Batch 10 | Loss: 0.7658 | Acc: 68.75%\n",
      "Ep 1 | Batch 15 | Loss: 0.7631 | Acc: 68.75%\n",
      "Ep 1 | Batch 20 | Loss: 0.7177 | Acc: 71.88%\n",
      "Ep 1 | Batch 25 | Loss: 0.6431 | Acc: 68.75%\n",
      "Ep 1 | Batch 30 | Loss: 0.8421 | Acc: 65.62%\n",
      "Ep 1 | Batch 35 | Loss: 0.8222 | Acc: 65.62%\n",
      "Ep 1 | Batch 40 | Loss: 0.8370 | Acc: 50.00%\n",
      "Ep 1 | Batch 45 | Loss: 0.5648 | Acc: 81.25%\n",
      "Ep 1 | Batch 50 | Loss: 0.8343 | Acc: 68.75%\n",
      "Ep 1 | Batch 55 | Loss: 0.7442 | Acc: 75.00%\n",
      "Ep 1 | Batch 60 | Loss: 0.7664 | Acc: 62.50%\n",
      "Ep 1 | Batch 65 | Loss: 0.6498 | Acc: 68.75%\n",
      "Ep 1 | Batch 70 | Loss: 0.8215 | Acc: 65.62%\n",
      "Ep 1 | Batch 75 | Loss: 0.7350 | Acc: 68.75%\n",
      "Ep 1 | Batch 80 | Loss: 0.5540 | Acc: 71.88%\n",
      "Ep 1 | Batch 85 | Loss: 0.8435 | Acc: 56.25%\n",
      "Ep 1 | Batch 90 | Loss: 0.7009 | Acc: 71.88%\n",
      "Ep 1 | Batch 95 | Loss: 0.6216 | Acc: 81.25%\n",
      "Ep 1 | Batch 100 | Loss: 0.5739 | Acc: 71.88%\n",
      "Ep 1 | Batch 105 | Loss: 0.8303 | Acc: 62.50%\n",
      "Ep 1 | Batch 110 | Loss: 0.5649 | Acc: 71.88%\n",
      "Ep 1 | Batch 115 | Loss: 0.7633 | Acc: 62.50%\n",
      "Ep 1 | Batch 120 | Loss: 0.6447 | Acc: 75.00%\n",
      "Ep 1 | Batch 125 | Loss: 0.7589 | Acc: 68.75%\n",
      "Ep 1 | Batch 130 | Loss: 0.6412 | Acc: 71.88%\n",
      "Ep 1 | Batch 135 | Loss: 0.5935 | Acc: 71.88%\n",
      "Ep 1 | Batch 140 | Loss: 0.6552 | Acc: 75.00%\n",
      "===> End Ep 1 | Avg Loss: 0.7198 | Avg Acc: 68.51%\n",
      "Ep 2 | Batch 0 | Loss: 0.5841 | Acc: 71.88%\n",
      "Ep 2 | Batch 5 | Loss: 0.5644 | Acc: 75.00%\n",
      "Ep 2 | Batch 10 | Loss: 0.5994 | Acc: 78.12%\n",
      "Ep 2 | Batch 15 | Loss: 0.7017 | Acc: 59.38%\n",
      "Ep 2 | Batch 20 | Loss: 0.7452 | Acc: 75.00%\n",
      "Ep 2 | Batch 25 | Loss: 0.5715 | Acc: 75.00%\n",
      "Ep 2 | Batch 30 | Loss: 0.5253 | Acc: 84.38%\n",
      "Ep 2 | Batch 35 | Loss: 0.6836 | Acc: 75.00%\n",
      "Ep 2 | Batch 40 | Loss: 0.5568 | Acc: 71.88%\n",
      "Ep 2 | Batch 45 | Loss: 0.7689 | Acc: 65.62%\n",
      "Ep 2 | Batch 50 | Loss: 0.7862 | Acc: 53.12%\n",
      "Ep 2 | Batch 55 | Loss: 0.4468 | Acc: 87.50%\n",
      "Ep 2 | Batch 60 | Loss: 0.6530 | Acc: 71.88%\n",
      "Ep 2 | Batch 65 | Loss: 0.6855 | Acc: 71.88%\n",
      "Ep 2 | Batch 70 | Loss: 0.7347 | Acc: 68.75%\n",
      "Ep 2 | Batch 75 | Loss: 0.7296 | Acc: 68.75%\n",
      "Ep 2 | Batch 80 | Loss: 0.6076 | Acc: 75.00%\n",
      "Ep 2 | Batch 85 | Loss: 0.7066 | Acc: 71.88%\n",
      "Ep 2 | Batch 90 | Loss: 0.6824 | Acc: 65.62%\n",
      "Ep 2 | Batch 95 | Loss: 0.6392 | Acc: 71.88%\n",
      "Ep 2 | Batch 100 | Loss: 0.6752 | Acc: 62.50%\n",
      "Ep 2 | Batch 105 | Loss: 0.6246 | Acc: 62.50%\n",
      "Ep 2 | Batch 110 | Loss: 0.9053 | Acc: 50.00%\n",
      "Ep 2 | Batch 115 | Loss: 0.7137 | Acc: 68.75%\n",
      "Ep 2 | Batch 120 | Loss: 0.6804 | Acc: 62.50%\n",
      "Ep 2 | Batch 125 | Loss: 0.3833 | Acc: 90.62%\n",
      "Ep 2 | Batch 130 | Loss: 0.6532 | Acc: 65.62%\n",
      "Ep 2 | Batch 135 | Loss: 0.4479 | Acc: 78.12%\n",
      "Ep 2 | Batch 140 | Loss: 0.7538 | Acc: 59.38%\n",
      "===> End Ep 2 | Avg Loss: 0.6238 | Avg Acc: 73.09%\n",
      "Ep 3 | Batch 0 | Loss: 0.4626 | Acc: 81.25%\n",
      "Ep 3 | Batch 5 | Loss: 0.4974 | Acc: 81.25%\n",
      "Ep 3 | Batch 10 | Loss: 0.5530 | Acc: 71.88%\n",
      "Ep 3 | Batch 15 | Loss: 0.6726 | Acc: 68.75%\n",
      "Ep 3 | Batch 20 | Loss: 0.6856 | Acc: 75.00%\n",
      "Ep 3 | Batch 25 | Loss: 0.7079 | Acc: 71.88%\n",
      "Ep 3 | Batch 30 | Loss: 0.7030 | Acc: 68.75%\n",
      "Ep 3 | Batch 35 | Loss: 0.6920 | Acc: 75.00%\n",
      "Ep 3 | Batch 40 | Loss: 0.5499 | Acc: 78.12%\n",
      "Ep 3 | Batch 45 | Loss: 0.6141 | Acc: 68.75%\n",
      "Ep 3 | Batch 50 | Loss: 0.7904 | Acc: 65.62%\n",
      "Ep 3 | Batch 55 | Loss: 0.4172 | Acc: 84.38%\n",
      "Ep 3 | Batch 60 | Loss: 0.4770 | Acc: 81.25%\n",
      "Ep 3 | Batch 65 | Loss: 0.6229 | Acc: 75.00%\n",
      "Ep 3 | Batch 70 | Loss: 0.5280 | Acc: 78.12%\n",
      "Ep 3 | Batch 75 | Loss: 0.6351 | Acc: 81.25%\n",
      "Ep 3 | Batch 80 | Loss: 0.5016 | Acc: 78.12%\n",
      "Ep 3 | Batch 85 | Loss: 0.3902 | Acc: 90.62%\n",
      "Ep 3 | Batch 90 | Loss: 0.5478 | Acc: 81.25%\n",
      "Ep 3 | Batch 95 | Loss: 0.5214 | Acc: 71.88%\n",
      "Ep 3 | Batch 100 | Loss: 0.5510 | Acc: 71.88%\n",
      "Ep 3 | Batch 105 | Loss: 0.5493 | Acc: 78.12%\n",
      "Ep 3 | Batch 110 | Loss: 0.5229 | Acc: 71.88%\n",
      "Ep 3 | Batch 115 | Loss: 0.5336 | Acc: 81.25%\n",
      "Ep 3 | Batch 120 | Loss: 0.7027 | Acc: 68.75%\n",
      "Ep 3 | Batch 125 | Loss: 0.4650 | Acc: 81.25%\n",
      "Ep 3 | Batch 130 | Loss: 0.4070 | Acc: 84.38%\n",
      "Ep 3 | Batch 135 | Loss: 0.4224 | Acc: 84.38%\n",
      "Ep 3 | Batch 140 | Loss: 0.4998 | Acc: 81.25%\n",
      "===> End Ep 3 | Avg Loss: 0.5715 | Avg Acc: 75.72%\n",
      "Ep 4 | Batch 0 | Loss: 0.6515 | Acc: 65.62%\n",
      "Ep 4 | Batch 5 | Loss: 0.4857 | Acc: 71.88%\n",
      "Ep 4 | Batch 10 | Loss: 0.6420 | Acc: 59.38%\n",
      "Ep 4 | Batch 15 | Loss: 0.6299 | Acc: 68.75%\n",
      "Ep 4 | Batch 20 | Loss: 0.5636 | Acc: 78.12%\n",
      "Ep 4 | Batch 25 | Loss: 0.6929 | Acc: 68.75%\n",
      "Ep 4 | Batch 30 | Loss: 0.5051 | Acc: 84.38%\n",
      "Ep 4 | Batch 35 | Loss: 0.4410 | Acc: 90.62%\n",
      "Ep 4 | Batch 40 | Loss: 0.4887 | Acc: 75.00%\n",
      "Ep 4 | Batch 45 | Loss: 0.5950 | Acc: 68.75%\n",
      "Ep 4 | Batch 50 | Loss: 0.6802 | Acc: 71.88%\n",
      "Ep 4 | Batch 55 | Loss: 0.4952 | Acc: 84.38%\n",
      "Ep 4 | Batch 60 | Loss: 0.5364 | Acc: 78.12%\n",
      "Ep 4 | Batch 65 | Loss: 0.4650 | Acc: 78.12%\n",
      "Ep 4 | Batch 70 | Loss: 0.4617 | Acc: 90.62%\n",
      "Ep 4 | Batch 75 | Loss: 0.6163 | Acc: 78.12%\n",
      "Ep 4 | Batch 80 | Loss: 0.5837 | Acc: 75.00%\n",
      "Ep 4 | Batch 85 | Loss: 0.4613 | Acc: 90.62%\n",
      "Ep 4 | Batch 90 | Loss: 0.6747 | Acc: 75.00%\n",
      "Ep 4 | Batch 95 | Loss: 0.4222 | Acc: 84.38%\n",
      "Ep 4 | Batch 100 | Loss: 0.5373 | Acc: 81.25%\n",
      "Ep 4 | Batch 105 | Loss: 0.3830 | Acc: 84.38%\n",
      "Ep 4 | Batch 110 | Loss: 0.6292 | Acc: 75.00%\n",
      "Ep 4 | Batch 115 | Loss: 0.4971 | Acc: 81.25%\n",
      "Ep 4 | Batch 120 | Loss: 0.7773 | Acc: 62.50%\n",
      "Ep 4 | Batch 125 | Loss: 0.7971 | Acc: 59.38%\n",
      "Ep 4 | Batch 130 | Loss: 0.4890 | Acc: 71.88%\n",
      "Ep 4 | Batch 135 | Loss: 0.7311 | Acc: 75.00%\n",
      "Ep 4 | Batch 140 | Loss: 0.6513 | Acc: 71.88%\n",
      "===> End Ep 4 | Avg Loss: 0.5529 | Avg Acc: 76.87%\n",
      "Ep 5 | Batch 0 | Loss: 0.5727 | Acc: 71.88%\n",
      "Ep 5 | Batch 5 | Loss: 0.4052 | Acc: 81.25%\n",
      "Ep 5 | Batch 10 | Loss: 0.5797 | Acc: 87.50%\n",
      "Ep 5 | Batch 15 | Loss: 0.7447 | Acc: 65.62%\n",
      "Ep 5 | Batch 20 | Loss: 0.4787 | Acc: 78.12%\n",
      "Ep 5 | Batch 25 | Loss: 0.5464 | Acc: 84.38%\n",
      "Ep 5 | Batch 30 | Loss: 0.6165 | Acc: 68.75%\n",
      "Ep 5 | Batch 35 | Loss: 0.6016 | Acc: 75.00%\n",
      "Ep 5 | Batch 40 | Loss: 0.5260 | Acc: 81.25%\n",
      "Ep 5 | Batch 45 | Loss: 0.6151 | Acc: 68.75%\n",
      "Ep 5 | Batch 50 | Loss: 0.5240 | Acc: 84.38%\n",
      "Ep 5 | Batch 55 | Loss: 0.3905 | Acc: 87.50%\n",
      "Ep 5 | Batch 60 | Loss: 0.6871 | Acc: 68.75%\n",
      "Ep 5 | Batch 65 | Loss: 0.5363 | Acc: 81.25%\n",
      "Ep 5 | Batch 70 | Loss: 0.6168 | Acc: 65.62%\n",
      "Ep 5 | Batch 75 | Loss: 0.5581 | Acc: 81.25%\n",
      "Ep 5 | Batch 80 | Loss: 0.5495 | Acc: 71.88%\n",
      "Ep 5 | Batch 85 | Loss: 0.6465 | Acc: 71.88%\n",
      "Ep 5 | Batch 90 | Loss: 0.4162 | Acc: 87.50%\n",
      "Ep 5 | Batch 95 | Loss: 0.6394 | Acc: 68.75%\n",
      "Ep 5 | Batch 100 | Loss: 0.5203 | Acc: 78.12%\n",
      "Ep 5 | Batch 105 | Loss: 0.4305 | Acc: 78.12%\n",
      "Ep 5 | Batch 110 | Loss: 0.4475 | Acc: 87.50%\n",
      "Ep 5 | Batch 115 | Loss: 0.6333 | Acc: 71.88%\n",
      "Ep 5 | Batch 120 | Loss: 0.5194 | Acc: 81.25%\n",
      "Ep 5 | Batch 125 | Loss: 0.4798 | Acc: 84.38%\n",
      "Ep 5 | Batch 130 | Loss: 0.4310 | Acc: 84.38%\n",
      "Ep 5 | Batch 135 | Loss: 0.4018 | Acc: 87.50%\n",
      "Ep 5 | Batch 140 | Loss: 0.6527 | Acc: 75.00%\n",
      "===> End Ep 5 | Avg Loss: 0.5278 | Avg Acc: 78.08%\n",
      "üîì Unfreezing Backbones...\n",
      "Ep 6 | Batch 0 | Loss: 0.8440 | Acc: 62.50%\n",
      "Ep 6 | Batch 5 | Loss: 0.5323 | Acc: 68.75%\n",
      "Ep 6 | Batch 10 | Loss: 0.3964 | Acc: 90.62%\n",
      "Ep 6 | Batch 15 | Loss: 0.4279 | Acc: 81.25%\n",
      "Ep 6 | Batch 20 | Loss: 0.5007 | Acc: 81.25%\n",
      "Ep 6 | Batch 25 | Loss: 0.3787 | Acc: 90.62%\n",
      "Ep 6 | Batch 30 | Loss: 0.6925 | Acc: 59.38%\n",
      "Ep 6 | Batch 35 | Loss: 0.3527 | Acc: 87.50%\n",
      "Ep 6 | Batch 40 | Loss: 0.3143 | Acc: 96.88%\n",
      "Ep 6 | Batch 45 | Loss: 0.5734 | Acc: 78.12%\n",
      "Ep 6 | Batch 50 | Loss: 0.4290 | Acc: 81.25%\n",
      "Ep 6 | Batch 55 | Loss: 0.5384 | Acc: 78.12%\n",
      "Ep 6 | Batch 60 | Loss: 0.4493 | Acc: 84.38%\n",
      "Ep 6 | Batch 65 | Loss: 0.3551 | Acc: 90.62%\n",
      "Ep 6 | Batch 70 | Loss: 0.2781 | Acc: 96.88%\n",
      "Ep 6 | Batch 75 | Loss: 0.4537 | Acc: 78.12%\n",
      "Ep 6 | Batch 80 | Loss: 0.5224 | Acc: 81.25%\n",
      "Ep 6 | Batch 85 | Loss: 0.3068 | Acc: 90.62%\n",
      "Ep 6 | Batch 90 | Loss: 0.2394 | Acc: 93.75%\n",
      "Ep 6 | Batch 95 | Loss: 0.6727 | Acc: 71.88%\n",
      "Ep 6 | Batch 100 | Loss: 0.3649 | Acc: 90.62%\n",
      "Ep 6 | Batch 105 | Loss: 0.4173 | Acc: 84.38%\n",
      "Ep 6 | Batch 110 | Loss: 0.5308 | Acc: 81.25%\n",
      "Ep 6 | Batch 115 | Loss: 0.4048 | Acc: 84.38%\n",
      "Ep 6 | Batch 120 | Loss: 0.5175 | Acc: 78.12%\n",
      "Ep 6 | Batch 125 | Loss: 0.4357 | Acc: 81.25%\n",
      "Ep 6 | Batch 130 | Loss: 0.4497 | Acc: 84.38%\n",
      "Ep 6 | Batch 135 | Loss: 0.4930 | Acc: 78.12%\n",
      "Ep 6 | Batch 140 | Loss: 0.4363 | Acc: 87.50%\n",
      "===> End Ep 6 | Avg Loss: 0.4728 | Avg Acc: 81.90%\n",
      "Ep 7 | Batch 0 | Loss: 0.3784 | Acc: 81.25%\n",
      "Ep 7 | Batch 5 | Loss: 0.2432 | Acc: 96.88%\n",
      "Ep 7 | Batch 10 | Loss: 0.3494 | Acc: 84.38%\n",
      "Ep 7 | Batch 15 | Loss: 0.5448 | Acc: 75.00%\n",
      "Ep 7 | Batch 20 | Loss: 0.5765 | Acc: 68.75%\n",
      "Ep 7 | Batch 25 | Loss: 0.4609 | Acc: 87.50%\n",
      "Ep 7 | Batch 30 | Loss: 0.6996 | Acc: 68.75%\n",
      "Ep 7 | Batch 35 | Loss: 0.6157 | Acc: 68.75%\n",
      "Ep 7 | Batch 40 | Loss: 0.5217 | Acc: 71.88%\n",
      "Ep 7 | Batch 45 | Loss: 0.4246 | Acc: 93.75%\n",
      "Ep 7 | Batch 50 | Loss: 0.3267 | Acc: 90.62%\n",
      "Ep 7 | Batch 55 | Loss: 0.4685 | Acc: 81.25%\n",
      "Ep 7 | Batch 60 | Loss: 0.4117 | Acc: 81.25%\n",
      "Ep 7 | Batch 65 | Loss: 0.6020 | Acc: 81.25%\n",
      "Ep 7 | Batch 70 | Loss: 0.4409 | Acc: 78.12%\n",
      "Ep 7 | Batch 75 | Loss: 0.5535 | Acc: 75.00%\n",
      "Ep 7 | Batch 80 | Loss: 0.5329 | Acc: 78.12%\n",
      "Ep 7 | Batch 85 | Loss: 0.4391 | Acc: 84.38%\n",
      "Ep 7 | Batch 90 | Loss: 0.5175 | Acc: 81.25%\n",
      "Ep 7 | Batch 95 | Loss: 0.4113 | Acc: 84.38%\n",
      "Ep 7 | Batch 100 | Loss: 0.4969 | Acc: 81.25%\n",
      "Ep 7 | Batch 105 | Loss: 0.2142 | Acc: 100.00%\n",
      "Ep 7 | Batch 110 | Loss: 0.2425 | Acc: 87.50%\n",
      "Ep 7 | Batch 115 | Loss: 0.2734 | Acc: 93.75%\n",
      "Ep 7 | Batch 120 | Loss: 0.4308 | Acc: 87.50%\n",
      "Ep 7 | Batch 125 | Loss: 0.5587 | Acc: 65.62%\n",
      "Ep 7 | Batch 130 | Loss: 0.2681 | Acc: 93.75%\n",
      "Ep 7 | Batch 135 | Loss: 0.4022 | Acc: 84.38%\n",
      "Ep 7 | Batch 140 | Loss: 0.2549 | Acc: 93.75%\n",
      "===> End Ep 7 | Avg Loss: 0.4353 | Avg Acc: 82.70%\n",
      "Ep 8 | Batch 0 | Loss: 0.4603 | Acc: 87.50%\n",
      "Ep 8 | Batch 5 | Loss: 0.4371 | Acc: 81.25%\n",
      "Ep 8 | Batch 10 | Loss: 0.3534 | Acc: 84.38%\n",
      "Ep 8 | Batch 15 | Loss: 0.2742 | Acc: 93.75%\n",
      "Ep 8 | Batch 20 | Loss: 0.4249 | Acc: 84.38%\n",
      "Ep 8 | Batch 25 | Loss: 0.3114 | Acc: 87.50%\n",
      "Ep 8 | Batch 30 | Loss: 0.3414 | Acc: 87.50%\n",
      "Ep 8 | Batch 35 | Loss: 0.3244 | Acc: 78.12%\n",
      "Ep 8 | Batch 40 | Loss: 0.2981 | Acc: 84.38%\n",
      "Ep 8 | Batch 45 | Loss: 0.4255 | Acc: 78.12%\n",
      "Ep 8 | Batch 50 | Loss: 0.3609 | Acc: 87.50%\n",
      "Ep 8 | Batch 55 | Loss: 0.3512 | Acc: 90.62%\n",
      "Ep 8 | Batch 60 | Loss: 0.5383 | Acc: 68.75%\n",
      "Ep 8 | Batch 65 | Loss: 0.4088 | Acc: 87.50%\n",
      "Ep 8 | Batch 70 | Loss: 0.3634 | Acc: 84.38%\n",
      "Ep 8 | Batch 75 | Loss: 0.4339 | Acc: 78.12%\n",
      "Ep 8 | Batch 80 | Loss: 0.4973 | Acc: 78.12%\n",
      "Ep 8 | Batch 85 | Loss: 0.3201 | Acc: 90.62%\n",
      "Ep 8 | Batch 90 | Loss: 0.3622 | Acc: 90.62%\n",
      "Ep 8 | Batch 95 | Loss: 0.2385 | Acc: 93.75%\n",
      "Ep 8 | Batch 100 | Loss: 0.2360 | Acc: 93.75%\n",
      "Ep 8 | Batch 105 | Loss: 0.4189 | Acc: 84.38%\n",
      "Ep 8 | Batch 110 | Loss: 0.3258 | Acc: 96.88%\n",
      "Ep 8 | Batch 115 | Loss: 0.3090 | Acc: 87.50%\n",
      "Ep 8 | Batch 120 | Loss: 0.4536 | Acc: 78.12%\n",
      "Ep 8 | Batch 125 | Loss: 0.2208 | Acc: 93.75%\n",
      "Ep 8 | Batch 130 | Loss: 0.4147 | Acc: 84.38%\n",
      "Ep 8 | Batch 135 | Loss: 0.3017 | Acc: 90.62%\n",
      "Ep 8 | Batch 140 | Loss: 0.4405 | Acc: 84.38%\n",
      "===> End Ep 8 | Avg Loss: 0.3909 | Avg Acc: 84.83%\n",
      "Ep 9 | Batch 0 | Loss: 0.3928 | Acc: 81.25%\n",
      "Ep 9 | Batch 5 | Loss: 0.1320 | Acc: 100.00%\n",
      "Ep 9 | Batch 10 | Loss: 0.4777 | Acc: 81.25%\n",
      "Ep 9 | Batch 15 | Loss: 0.5340 | Acc: 75.00%\n",
      "Ep 9 | Batch 20 | Loss: 0.3283 | Acc: 87.50%\n",
      "Ep 9 | Batch 25 | Loss: 0.3904 | Acc: 81.25%\n",
      "Ep 9 | Batch 30 | Loss: 0.5061 | Acc: 75.00%\n",
      "Ep 9 | Batch 35 | Loss: 0.3163 | Acc: 93.75%\n",
      "Ep 9 | Batch 40 | Loss: 0.2668 | Acc: 93.75%\n",
      "Ep 9 | Batch 45 | Loss: 0.3301 | Acc: 90.62%\n",
      "Ep 9 | Batch 50 | Loss: 0.4271 | Acc: 78.12%\n",
      "Ep 9 | Batch 55 | Loss: 0.3592 | Acc: 87.50%\n",
      "Ep 9 | Batch 60 | Loss: 0.3330 | Acc: 87.50%\n",
      "Ep 9 | Batch 65 | Loss: 0.3653 | Acc: 87.50%\n",
      "Ep 9 | Batch 70 | Loss: 0.3374 | Acc: 90.62%\n",
      "Ep 9 | Batch 75 | Loss: 0.5458 | Acc: 78.12%\n",
      "Ep 9 | Batch 80 | Loss: 0.3543 | Acc: 90.62%\n",
      "Ep 9 | Batch 85 | Loss: 0.4497 | Acc: 84.38%\n",
      "Ep 9 | Batch 90 | Loss: 0.5289 | Acc: 75.00%\n",
      "Ep 9 | Batch 95 | Loss: 0.3714 | Acc: 84.38%\n",
      "Ep 9 | Batch 100 | Loss: 0.3523 | Acc: 87.50%\n",
      "Ep 9 | Batch 105 | Loss: 0.4918 | Acc: 75.00%\n",
      "Ep 9 | Batch 110 | Loss: 0.2945 | Acc: 93.75%\n",
      "Ep 9 | Batch 115 | Loss: 0.4450 | Acc: 78.12%\n",
      "Ep 9 | Batch 120 | Loss: 0.2637 | Acc: 90.62%\n",
      "Ep 9 | Batch 125 | Loss: 0.4023 | Acc: 84.38%\n",
      "Ep 9 | Batch 130 | Loss: 0.2785 | Acc: 84.38%\n",
      "Ep 9 | Batch 135 | Loss: 0.1813 | Acc: 96.88%\n",
      "Ep 9 | Batch 140 | Loss: 0.3183 | Acc: 93.75%\n",
      "===> End Ep 9 | Avg Loss: 0.3494 | Avg Acc: 87.02%\n",
      "Ep 10 | Batch 0 | Loss: 0.2646 | Acc: 93.75%\n",
      "Ep 10 | Batch 5 | Loss: 0.4254 | Acc: 81.25%\n",
      "Ep 10 | Batch 10 | Loss: 0.4327 | Acc: 84.38%\n",
      "Ep 10 | Batch 15 | Loss: 0.2237 | Acc: 96.88%\n",
      "Ep 10 | Batch 20 | Loss: 0.4797 | Acc: 75.00%\n",
      "Ep 10 | Batch 25 | Loss: 0.3640 | Acc: 84.38%\n",
      "Ep 10 | Batch 30 | Loss: 0.2497 | Acc: 90.62%\n",
      "Ep 10 | Batch 35 | Loss: 0.4735 | Acc: 84.38%\n",
      "Ep 10 | Batch 40 | Loss: 0.3325 | Acc: 84.38%\n",
      "Ep 10 | Batch 45 | Loss: 0.4565 | Acc: 75.00%\n",
      "Ep 10 | Batch 50 | Loss: 0.3330 | Acc: 87.50%\n",
      "Ep 10 | Batch 55 | Loss: 0.3797 | Acc: 90.62%\n",
      "Ep 10 | Batch 60 | Loss: 0.5941 | Acc: 75.00%\n",
      "Ep 10 | Batch 65 | Loss: 0.1744 | Acc: 93.75%\n",
      "Ep 10 | Batch 70 | Loss: 0.2517 | Acc: 93.75%\n",
      "Ep 10 | Batch 75 | Loss: 0.2627 | Acc: 93.75%\n",
      "Ep 10 | Batch 80 | Loss: 0.2826 | Acc: 87.50%\n",
      "Ep 10 | Batch 85 | Loss: 0.4505 | Acc: 81.25%\n",
      "Ep 10 | Batch 90 | Loss: 0.3716 | Acc: 84.38%\n",
      "Ep 10 | Batch 95 | Loss: 0.1782 | Acc: 93.75%\n",
      "Ep 10 | Batch 100 | Loss: 0.3198 | Acc: 84.38%\n",
      "Ep 10 | Batch 105 | Loss: 0.2517 | Acc: 90.62%\n",
      "Ep 10 | Batch 110 | Loss: 0.1215 | Acc: 96.88%\n",
      "Ep 10 | Batch 115 | Loss: 0.2547 | Acc: 96.88%\n",
      "Ep 10 | Batch 120 | Loss: 0.3309 | Acc: 84.38%\n",
      "Ep 10 | Batch 125 | Loss: 0.3245 | Acc: 93.75%\n",
      "Ep 10 | Batch 130 | Loss: 0.3391 | Acc: 93.75%\n",
      "Ep 10 | Batch 135 | Loss: 0.1432 | Acc: 96.88%\n",
      "Ep 10 | Batch 140 | Loss: 0.4696 | Acc: 78.12%\n",
      "===> End Ep 10 | Avg Loss: 0.3430 | Avg Acc: 87.93%\n",
      "Ep 11 | Batch 0 | Loss: 0.3832 | Acc: 87.50%\n",
      "Ep 11 | Batch 5 | Loss: 0.2872 | Acc: 87.50%\n",
      "Ep 11 | Batch 10 | Loss: 0.4082 | Acc: 87.50%\n",
      "Ep 11 | Batch 15 | Loss: 0.2255 | Acc: 93.75%\n",
      "Ep 11 | Batch 20 | Loss: 0.3467 | Acc: 90.62%\n",
      "Ep 11 | Batch 25 | Loss: 0.3392 | Acc: 87.50%\n",
      "Ep 11 | Batch 30 | Loss: 0.3140 | Acc: 90.62%\n",
      "Ep 11 | Batch 35 | Loss: 0.2776 | Acc: 90.62%\n",
      "Ep 11 | Batch 40 | Loss: 0.3773 | Acc: 87.50%\n",
      "Ep 11 | Batch 45 | Loss: 0.4291 | Acc: 84.38%\n",
      "Ep 11 | Batch 50 | Loss: 0.2281 | Acc: 96.88%\n",
      "Ep 11 | Batch 55 | Loss: 0.4765 | Acc: 78.12%\n",
      "Ep 11 | Batch 60 | Loss: 0.3058 | Acc: 93.75%\n",
      "Ep 11 | Batch 65 | Loss: 0.2566 | Acc: 90.62%\n",
      "Ep 11 | Batch 70 | Loss: 0.2620 | Acc: 90.62%\n",
      "Ep 11 | Batch 75 | Loss: 0.2389 | Acc: 93.75%\n",
      "Ep 11 | Batch 80 | Loss: 0.3438 | Acc: 84.38%\n",
      "Ep 11 | Batch 85 | Loss: 0.2755 | Acc: 87.50%\n",
      "Ep 11 | Batch 90 | Loss: 0.3575 | Acc: 90.62%\n",
      "Ep 11 | Batch 95 | Loss: 0.3465 | Acc: 81.25%\n",
      "Ep 11 | Batch 100 | Loss: 0.3061 | Acc: 87.50%\n",
      "Ep 11 | Batch 105 | Loss: 0.3395 | Acc: 87.50%\n",
      "Ep 11 | Batch 110 | Loss: 0.3735 | Acc: 87.50%\n",
      "Ep 11 | Batch 115 | Loss: 0.2952 | Acc: 87.50%\n",
      "Ep 11 | Batch 120 | Loss: 0.2421 | Acc: 87.50%\n",
      "Ep 11 | Batch 125 | Loss: 0.3241 | Acc: 87.50%\n",
      "Ep 11 | Batch 130 | Loss: 0.1672 | Acc: 96.88%\n",
      "Ep 11 | Batch 135 | Loss: 0.3615 | Acc: 81.25%\n",
      "Ep 11 | Batch 140 | Loss: 0.3389 | Acc: 93.75%\n",
      "===> End Ep 11 | Avg Loss: 0.3182 | Avg Acc: 88.69%\n",
      "Ep 12 | Batch 0 | Loss: 0.2739 | Acc: 87.50%\n",
      "Ep 12 | Batch 5 | Loss: 0.2068 | Acc: 96.88%\n",
      "Ep 12 | Batch 10 | Loss: 0.3299 | Acc: 90.62%\n",
      "Ep 12 | Batch 15 | Loss: 0.3522 | Acc: 87.50%\n",
      "Ep 12 | Batch 20 | Loss: 0.6648 | Acc: 75.00%\n",
      "Ep 12 | Batch 25 | Loss: 0.2476 | Acc: 93.75%\n",
      "Ep 12 | Batch 30 | Loss: 0.3305 | Acc: 90.62%\n",
      "Ep 12 | Batch 35 | Loss: 0.3667 | Acc: 87.50%\n",
      "Ep 12 | Batch 40 | Loss: 0.2622 | Acc: 90.62%\n",
      "Ep 12 | Batch 45 | Loss: 0.4211 | Acc: 93.75%\n",
      "Ep 12 | Batch 50 | Loss: 0.3365 | Acc: 87.50%\n",
      "Ep 12 | Batch 55 | Loss: 0.3614 | Acc: 84.38%\n",
      "Ep 12 | Batch 60 | Loss: 0.2829 | Acc: 87.50%\n",
      "Ep 12 | Batch 65 | Loss: 0.2606 | Acc: 90.62%\n",
      "Ep 12 | Batch 70 | Loss: 0.4456 | Acc: 84.38%\n",
      "Ep 12 | Batch 75 | Loss: 0.2743 | Acc: 90.62%\n",
      "Ep 12 | Batch 80 | Loss: 0.4081 | Acc: 87.50%\n",
      "Ep 12 | Batch 85 | Loss: 0.3134 | Acc: 90.62%\n",
      "Ep 12 | Batch 90 | Loss: 0.2288 | Acc: 87.50%\n",
      "Ep 12 | Batch 95 | Loss: 0.1817 | Acc: 96.88%\n",
      "Ep 12 | Batch 100 | Loss: 0.5360 | Acc: 78.12%\n",
      "Ep 12 | Batch 105 | Loss: 0.2846 | Acc: 90.62%\n",
      "Ep 12 | Batch 110 | Loss: 0.3739 | Acc: 78.12%\n",
      "Ep 12 | Batch 115 | Loss: 0.4804 | Acc: 78.12%\n",
      "Ep 12 | Batch 120 | Loss: 0.2156 | Acc: 96.88%\n",
      "Ep 12 | Batch 125 | Loss: 0.3783 | Acc: 84.38%\n",
      "Ep 12 | Batch 130 | Loss: 0.2097 | Acc: 96.88%\n",
      "Ep 12 | Batch 135 | Loss: 0.5680 | Acc: 84.38%\n",
      "Ep 12 | Batch 140 | Loss: 0.1993 | Acc: 100.00%\n",
      "===> End Ep 12 | Avg Loss: 0.3141 | Avg Acc: 88.95%\n",
      "Ep 13 | Batch 0 | Loss: 0.1566 | Acc: 100.00%\n",
      "Ep 13 | Batch 5 | Loss: 0.3715 | Acc: 84.38%\n",
      "Ep 13 | Batch 10 | Loss: 0.2376 | Acc: 90.62%\n",
      "Ep 13 | Batch 15 | Loss: 0.4046 | Acc: 87.50%\n",
      "Ep 13 | Batch 20 | Loss: 0.2274 | Acc: 90.62%\n",
      "Ep 13 | Batch 25 | Loss: 0.3498 | Acc: 93.75%\n",
      "Ep 13 | Batch 30 | Loss: 0.3411 | Acc: 90.62%\n",
      "Ep 13 | Batch 35 | Loss: 0.3788 | Acc: 84.38%\n",
      "Ep 13 | Batch 40 | Loss: 0.1201 | Acc: 96.88%\n",
      "Ep 13 | Batch 45 | Loss: 0.2028 | Acc: 90.62%\n",
      "Ep 13 | Batch 50 | Loss: 0.1174 | Acc: 96.88%\n",
      "Ep 13 | Batch 55 | Loss: 0.1369 | Acc: 100.00%\n",
      "Ep 13 | Batch 60 | Loss: 0.2899 | Acc: 87.50%\n",
      "Ep 13 | Batch 65 | Loss: 0.3629 | Acc: 87.50%\n",
      "Ep 13 | Batch 70 | Loss: 0.3210 | Acc: 84.38%\n",
      "Ep 13 | Batch 75 | Loss: 0.2487 | Acc: 93.75%\n",
      "Ep 13 | Batch 80 | Loss: 0.3173 | Acc: 93.75%\n",
      "Ep 13 | Batch 85 | Loss: 0.2407 | Acc: 93.75%\n",
      "Ep 13 | Batch 90 | Loss: 0.1856 | Acc: 96.88%\n",
      "Ep 13 | Batch 95 | Loss: 0.4034 | Acc: 78.12%\n",
      "Ep 13 | Batch 100 | Loss: 0.2711 | Acc: 90.62%\n",
      "Ep 13 | Batch 105 | Loss: 0.2816 | Acc: 93.75%\n",
      "Ep 13 | Batch 110 | Loss: 0.2131 | Acc: 93.75%\n",
      "Ep 13 | Batch 115 | Loss: 0.2728 | Acc: 90.62%\n",
      "Ep 13 | Batch 120 | Loss: 0.3333 | Acc: 84.38%\n",
      "Ep 13 | Batch 125 | Loss: 0.1230 | Acc: 96.88%\n",
      "Ep 13 | Batch 130 | Loss: 0.2854 | Acc: 90.62%\n",
      "Ep 13 | Batch 135 | Loss: 0.3789 | Acc: 87.50%\n",
      "Ep 13 | Batch 140 | Loss: 0.1356 | Acc: 93.75%\n",
      "===> End Ep 13 | Avg Loss: 0.2850 | Avg Acc: 90.19%\n",
      "Ep 14 | Batch 0 | Loss: 0.1874 | Acc: 93.75%\n",
      "Ep 14 | Batch 5 | Loss: 0.5723 | Acc: 75.00%\n",
      "Ep 14 | Batch 10 | Loss: 0.1875 | Acc: 93.75%\n",
      "Ep 14 | Batch 15 | Loss: 0.4391 | Acc: 84.38%\n",
      "Ep 14 | Batch 20 | Loss: 0.1875 | Acc: 90.62%\n",
      "Ep 14 | Batch 25 | Loss: 0.2232 | Acc: 87.50%\n",
      "Ep 14 | Batch 30 | Loss: 0.2755 | Acc: 93.75%\n",
      "Ep 14 | Batch 35 | Loss: 0.2383 | Acc: 96.88%\n",
      "Ep 14 | Batch 40 | Loss: 0.4100 | Acc: 81.25%\n",
      "Ep 14 | Batch 45 | Loss: 0.1288 | Acc: 96.88%\n",
      "Ep 14 | Batch 50 | Loss: 0.2645 | Acc: 90.62%\n",
      "Ep 14 | Batch 55 | Loss: 0.1283 | Acc: 100.00%\n",
      "Ep 14 | Batch 60 | Loss: 0.2390 | Acc: 90.62%\n",
      "Ep 14 | Batch 65 | Loss: 0.3370 | Acc: 87.50%\n",
      "Ep 14 | Batch 70 | Loss: 0.2055 | Acc: 96.88%\n",
      "Ep 14 | Batch 75 | Loss: 0.2275 | Acc: 93.75%\n",
      "Ep 14 | Batch 80 | Loss: 0.3481 | Acc: 84.38%\n",
      "Ep 14 | Batch 85 | Loss: 0.1544 | Acc: 96.88%\n",
      "Ep 14 | Batch 90 | Loss: 0.2248 | Acc: 93.75%\n",
      "Ep 14 | Batch 95 | Loss: 0.3777 | Acc: 87.50%\n",
      "Ep 14 | Batch 100 | Loss: 0.3209 | Acc: 87.50%\n",
      "Ep 14 | Batch 105 | Loss: 0.3016 | Acc: 90.62%\n",
      "Ep 14 | Batch 110 | Loss: 0.2015 | Acc: 96.88%\n",
      "Ep 14 | Batch 115 | Loss: 0.5067 | Acc: 71.88%\n",
      "Ep 14 | Batch 120 | Loss: 0.1204 | Acc: 96.88%\n",
      "Ep 14 | Batch 125 | Loss: 0.4582 | Acc: 81.25%\n",
      "Ep 14 | Batch 130 | Loss: 0.3971 | Acc: 84.38%\n",
      "Ep 14 | Batch 135 | Loss: 0.2936 | Acc: 90.62%\n",
      "Ep 14 | Batch 140 | Loss: 0.4092 | Acc: 87.50%\n",
      "===> End Ep 14 | Avg Loss: 0.2921 | Avg Acc: 89.39%\n",
      "Ep 15 | Batch 0 | Loss: 0.2802 | Acc: 87.50%\n",
      "Ep 15 | Batch 5 | Loss: 0.2904 | Acc: 90.62%\n",
      "Ep 15 | Batch 10 | Loss: 0.2861 | Acc: 93.75%\n",
      "Ep 15 | Batch 15 | Loss: 0.1544 | Acc: 90.62%\n",
      "Ep 15 | Batch 20 | Loss: 0.2750 | Acc: 93.75%\n",
      "Ep 15 | Batch 25 | Loss: 0.2271 | Acc: 87.50%\n",
      "Ep 15 | Batch 30 | Loss: 0.3890 | Acc: 84.38%\n",
      "Ep 15 | Batch 35 | Loss: 0.4303 | Acc: 81.25%\n",
      "Ep 15 | Batch 40 | Loss: 0.2207 | Acc: 93.75%\n",
      "Ep 15 | Batch 45 | Loss: 0.3032 | Acc: 93.75%\n",
      "Ep 15 | Batch 50 | Loss: 0.4730 | Acc: 87.50%\n",
      "Ep 15 | Batch 55 | Loss: 0.3669 | Acc: 84.38%\n",
      "Ep 15 | Batch 60 | Loss: 0.2875 | Acc: 90.62%\n",
      "Ep 15 | Batch 65 | Loss: 0.2505 | Acc: 96.88%\n",
      "Ep 15 | Batch 70 | Loss: 0.4040 | Acc: 84.38%\n",
      "Ep 15 | Batch 75 | Loss: 0.2259 | Acc: 90.62%\n",
      "Ep 15 | Batch 80 | Loss: 0.2013 | Acc: 100.00%\n",
      "Ep 15 | Batch 85 | Loss: 0.3264 | Acc: 87.50%\n",
      "Ep 15 | Batch 90 | Loss: 0.1681 | Acc: 93.75%\n",
      "Ep 15 | Batch 95 | Loss: 0.2148 | Acc: 100.00%\n",
      "Ep 15 | Batch 100 | Loss: 0.2241 | Acc: 93.75%\n",
      "Ep 15 | Batch 105 | Loss: 0.3333 | Acc: 81.25%\n",
      "Ep 15 | Batch 110 | Loss: 0.2042 | Acc: 93.75%\n",
      "Ep 15 | Batch 115 | Loss: 0.4043 | Acc: 84.38%\n",
      "Ep 15 | Batch 120 | Loss: 0.2790 | Acc: 87.50%\n",
      "Ep 15 | Batch 125 | Loss: 0.4174 | Acc: 78.12%\n",
      "Ep 15 | Batch 130 | Loss: 0.2926 | Acc: 90.62%\n",
      "Ep 15 | Batch 135 | Loss: 0.3842 | Acc: 87.50%\n",
      "Ep 15 | Batch 140 | Loss: 0.3135 | Acc: 87.50%\n",
      "===> End Ep 15 | Avg Loss: 0.2858 | Avg Acc: 90.13%\n",
      "Ep 16 | Batch 0 | Loss: 0.2224 | Acc: 96.88%\n",
      "Ep 16 | Batch 5 | Loss: 0.2087 | Acc: 96.88%\n",
      "Ep 16 | Batch 10 | Loss: 0.2077 | Acc: 96.88%\n",
      "Ep 16 | Batch 15 | Loss: 0.2564 | Acc: 90.62%\n",
      "Ep 16 | Batch 20 | Loss: 0.3171 | Acc: 93.75%\n",
      "Ep 16 | Batch 25 | Loss: 0.2226 | Acc: 87.50%\n",
      "Ep 16 | Batch 30 | Loss: 0.2591 | Acc: 90.62%\n",
      "Ep 16 | Batch 35 | Loss: 0.1980 | Acc: 90.62%\n",
      "Ep 16 | Batch 40 | Loss: 0.4885 | Acc: 78.12%\n",
      "Ep 16 | Batch 45 | Loss: 0.1294 | Acc: 93.75%\n",
      "Ep 16 | Batch 50 | Loss: 0.2429 | Acc: 93.75%\n",
      "Ep 16 | Batch 55 | Loss: 0.2210 | Acc: 93.75%\n",
      "Ep 16 | Batch 60 | Loss: 0.3265 | Acc: 90.62%\n",
      "Ep 16 | Batch 65 | Loss: 0.2509 | Acc: 84.38%\n",
      "Ep 16 | Batch 70 | Loss: 0.2133 | Acc: 93.75%\n",
      "Ep 16 | Batch 75 | Loss: 0.1999 | Acc: 93.75%\n",
      "Ep 16 | Batch 80 | Loss: 0.2510 | Acc: 87.50%\n",
      "Ep 16 | Batch 85 | Loss: 0.2410 | Acc: 90.62%\n",
      "Ep 16 | Batch 90 | Loss: 0.2195 | Acc: 100.00%\n",
      "Ep 16 | Batch 95 | Loss: 0.3298 | Acc: 87.50%\n",
      "Ep 16 | Batch 100 | Loss: 0.2816 | Acc: 93.75%\n",
      "Ep 16 | Batch 105 | Loss: 0.1867 | Acc: 93.75%\n",
      "Ep 16 | Batch 110 | Loss: 0.1005 | Acc: 100.00%\n",
      "Ep 16 | Batch 115 | Loss: 0.2314 | Acc: 90.62%\n",
      "Ep 16 | Batch 120 | Loss: 0.1488 | Acc: 96.88%\n",
      "Ep 16 | Batch 125 | Loss: 0.2320 | Acc: 90.62%\n",
      "Ep 16 | Batch 130 | Loss: 0.3900 | Acc: 87.50%\n",
      "Ep 16 | Batch 135 | Loss: 0.1800 | Acc: 96.88%\n",
      "Ep 16 | Batch 140 | Loss: 0.0906 | Acc: 96.88%\n",
      "===> End Ep 16 | Avg Loss: 0.2454 | Avg Acc: 91.86%\n",
      "Ep 17 | Batch 0 | Loss: 0.2284 | Acc: 96.88%\n",
      "Ep 17 | Batch 5 | Loss: 0.1692 | Acc: 93.75%\n",
      "Ep 17 | Batch 10 | Loss: 0.3391 | Acc: 84.38%\n",
      "Ep 17 | Batch 15 | Loss: 0.2385 | Acc: 93.75%\n",
      "Ep 17 | Batch 20 | Loss: 0.1789 | Acc: 93.75%\n",
      "Ep 17 | Batch 25 | Loss: 0.2726 | Acc: 96.88%\n",
      "Ep 17 | Batch 30 | Loss: 0.4971 | Acc: 68.75%\n",
      "Ep 17 | Batch 35 | Loss: 0.1916 | Acc: 90.62%\n",
      "Ep 17 | Batch 40 | Loss: 0.2129 | Acc: 100.00%\n",
      "Ep 17 | Batch 45 | Loss: 0.3471 | Acc: 84.38%\n",
      "Ep 17 | Batch 50 | Loss: 0.1672 | Acc: 96.88%\n",
      "Ep 17 | Batch 55 | Loss: 0.2608 | Acc: 87.50%\n",
      "Ep 17 | Batch 60 | Loss: 0.2318 | Acc: 90.62%\n",
      "Ep 17 | Batch 65 | Loss: 0.1734 | Acc: 93.75%\n",
      "Ep 17 | Batch 70 | Loss: 0.1755 | Acc: 96.88%\n",
      "Ep 17 | Batch 75 | Loss: 0.1367 | Acc: 100.00%\n",
      "Ep 17 | Batch 80 | Loss: 0.3431 | Acc: 93.75%\n",
      "Ep 17 | Batch 85 | Loss: 0.4761 | Acc: 84.38%\n",
      "Ep 17 | Batch 90 | Loss: 0.2152 | Acc: 96.88%\n",
      "Ep 17 | Batch 95 | Loss: 0.3197 | Acc: 93.75%\n",
      "Ep 17 | Batch 100 | Loss: 0.2362 | Acc: 90.62%\n",
      "Ep 17 | Batch 105 | Loss: 0.1991 | Acc: 93.75%\n",
      "Ep 17 | Batch 110 | Loss: 0.2179 | Acc: 93.75%\n",
      "Ep 17 | Batch 115 | Loss: 0.2401 | Acc: 93.75%\n",
      "Ep 17 | Batch 120 | Loss: 0.2987 | Acc: 90.62%\n",
      "Ep 17 | Batch 125 | Loss: 0.1558 | Acc: 100.00%\n",
      "Ep 17 | Batch 130 | Loss: 0.3543 | Acc: 81.25%\n",
      "Ep 17 | Batch 135 | Loss: 0.3718 | Acc: 81.25%\n",
      "Ep 17 | Batch 140 | Loss: 0.2282 | Acc: 93.75%\n",
      "===> End Ep 17 | Avg Loss: 0.2610 | Avg Acc: 90.95%\n",
      "Ep 18 | Batch 0 | Loss: 0.2866 | Acc: 87.50%\n",
      "Ep 18 | Batch 5 | Loss: 0.2952 | Acc: 93.75%\n",
      "Ep 18 | Batch 10 | Loss: 0.2534 | Acc: 93.75%\n",
      "Ep 18 | Batch 15 | Loss: 0.2687 | Acc: 87.50%\n",
      "Ep 18 | Batch 20 | Loss: 0.1412 | Acc: 100.00%\n",
      "Ep 18 | Batch 25 | Loss: 0.1664 | Acc: 93.75%\n",
      "Ep 18 | Batch 30 | Loss: 0.3462 | Acc: 84.38%\n",
      "Ep 18 | Batch 35 | Loss: 0.2406 | Acc: 90.62%\n",
      "Ep 18 | Batch 40 | Loss: 0.4496 | Acc: 84.38%\n",
      "Ep 18 | Batch 45 | Loss: 0.2203 | Acc: 90.62%\n",
      "Ep 18 | Batch 50 | Loss: 0.2000 | Acc: 90.62%\n",
      "Ep 18 | Batch 55 | Loss: 0.2207 | Acc: 90.62%\n",
      "Ep 18 | Batch 60 | Loss: 0.3252 | Acc: 87.50%\n",
      "Ep 18 | Batch 65 | Loss: 0.3679 | Acc: 81.25%\n",
      "Ep 18 | Batch 70 | Loss: 0.1462 | Acc: 93.75%\n",
      "Ep 18 | Batch 75 | Loss: 0.2219 | Acc: 93.75%\n",
      "Ep 18 | Batch 80 | Loss: 0.2034 | Acc: 90.62%\n",
      "Ep 18 | Batch 85 | Loss: 0.2628 | Acc: 87.50%\n",
      "Ep 18 | Batch 90 | Loss: 0.3575 | Acc: 87.50%\n",
      "Ep 18 | Batch 95 | Loss: 0.3237 | Acc: 87.50%\n",
      "Ep 18 | Batch 100 | Loss: 0.1120 | Acc: 96.88%\n",
      "Ep 18 | Batch 105 | Loss: 0.2240 | Acc: 93.75%\n",
      "Ep 18 | Batch 110 | Loss: 0.2698 | Acc: 93.75%\n",
      "Ep 18 | Batch 115 | Loss: 0.2338 | Acc: 100.00%\n",
      "Ep 18 | Batch 120 | Loss: 0.1101 | Acc: 96.88%\n",
      "Ep 18 | Batch 125 | Loss: 0.2019 | Acc: 87.50%\n",
      "Ep 18 | Batch 130 | Loss: 0.1877 | Acc: 90.62%\n",
      "Ep 18 | Batch 135 | Loss: 0.2110 | Acc: 93.75%\n",
      "Ep 18 | Batch 140 | Loss: 0.1773 | Acc: 93.75%\n",
      "===> End Ep 18 | Avg Loss: 0.2576 | Avg Acc: 91.58%\n",
      "Ep 19 | Batch 0 | Loss: 0.2239 | Acc: 96.88%\n",
      "Ep 19 | Batch 5 | Loss: 0.2818 | Acc: 90.62%\n",
      "Ep 19 | Batch 10 | Loss: 0.1504 | Acc: 93.75%\n",
      "Ep 19 | Batch 15 | Loss: 0.2038 | Acc: 93.75%\n",
      "Ep 19 | Batch 20 | Loss: 0.2714 | Acc: 96.88%\n",
      "Ep 19 | Batch 25 | Loss: 0.2561 | Acc: 90.62%\n",
      "Ep 19 | Batch 30 | Loss: 0.2194 | Acc: 96.88%\n",
      "Ep 19 | Batch 35 | Loss: 0.3362 | Acc: 84.38%\n",
      "Ep 19 | Batch 40 | Loss: 0.3087 | Acc: 87.50%\n",
      "Ep 19 | Batch 45 | Loss: 0.2555 | Acc: 90.62%\n",
      "Ep 19 | Batch 50 | Loss: 0.3661 | Acc: 84.38%\n",
      "Ep 19 | Batch 55 | Loss: 0.2399 | Acc: 93.75%\n",
      "Ep 19 | Batch 60 | Loss: 0.2569 | Acc: 93.75%\n",
      "Ep 19 | Batch 65 | Loss: 0.3271 | Acc: 90.62%\n",
      "Ep 19 | Batch 70 | Loss: 0.3021 | Acc: 90.62%\n",
      "Ep 19 | Batch 75 | Loss: 0.1806 | Acc: 96.88%\n",
      "Ep 19 | Batch 80 | Loss: 0.2651 | Acc: 93.75%\n",
      "Ep 19 | Batch 85 | Loss: 0.2727 | Acc: 93.75%\n",
      "Ep 19 | Batch 90 | Loss: 0.2526 | Acc: 93.75%\n",
      "Ep 19 | Batch 95 | Loss: 0.2895 | Acc: 87.50%\n",
      "Ep 19 | Batch 100 | Loss: 0.2727 | Acc: 87.50%\n",
      "Ep 19 | Batch 105 | Loss: 0.1819 | Acc: 93.75%\n",
      "Ep 19 | Batch 110 | Loss: 0.2519 | Acc: 87.50%\n",
      "Ep 19 | Batch 115 | Loss: 0.2801 | Acc: 90.62%\n",
      "Ep 19 | Batch 120 | Loss: 0.4012 | Acc: 87.50%\n",
      "Ep 19 | Batch 125 | Loss: 0.1273 | Acc: 96.88%\n",
      "Ep 19 | Batch 130 | Loss: 0.3157 | Acc: 87.50%\n",
      "Ep 19 | Batch 135 | Loss: 0.2300 | Acc: 93.75%\n",
      "Ep 19 | Batch 140 | Loss: 0.3270 | Acc: 84.38%\n",
      "===> End Ep 19 | Avg Loss: 0.2464 | Avg Acc: 91.95%\n",
      "Ep 20 | Batch 0 | Loss: 0.2100 | Acc: 100.00%\n",
      "Ep 20 | Batch 5 | Loss: 0.2357 | Acc: 100.00%\n",
      "Ep 20 | Batch 10 | Loss: 0.2077 | Acc: 90.62%\n",
      "Ep 20 | Batch 15 | Loss: 0.3229 | Acc: 87.50%\n",
      "Ep 20 | Batch 20 | Loss: 0.1882 | Acc: 96.88%\n",
      "Ep 20 | Batch 25 | Loss: 0.4397 | Acc: 78.12%\n",
      "Ep 20 | Batch 30 | Loss: 0.1708 | Acc: 100.00%\n",
      "Ep 20 | Batch 35 | Loss: 0.3540 | Acc: 81.25%\n",
      "Ep 20 | Batch 40 | Loss: 0.3280 | Acc: 87.50%\n",
      "Ep 20 | Batch 45 | Loss: 0.2484 | Acc: 90.62%\n",
      "Ep 20 | Batch 50 | Loss: 0.1516 | Acc: 96.88%\n",
      "Ep 20 | Batch 55 | Loss: 0.2476 | Acc: 93.75%\n",
      "Ep 20 | Batch 60 | Loss: 0.1583 | Acc: 96.88%\n",
      "Ep 20 | Batch 65 | Loss: 0.2813 | Acc: 90.62%\n",
      "Ep 20 | Batch 70 | Loss: 0.1868 | Acc: 93.75%\n",
      "Ep 20 | Batch 75 | Loss: 0.1507 | Acc: 100.00%\n",
      "Ep 20 | Batch 80 | Loss: 0.0759 | Acc: 100.00%\n",
      "Ep 20 | Batch 85 | Loss: 0.1513 | Acc: 100.00%\n",
      "Ep 20 | Batch 90 | Loss: 0.3273 | Acc: 90.62%\n",
      "Ep 20 | Batch 95 | Loss: 0.1814 | Acc: 93.75%\n",
      "Ep 20 | Batch 100 | Loss: 0.2510 | Acc: 93.75%\n",
      "Ep 20 | Batch 105 | Loss: 0.2383 | Acc: 87.50%\n",
      "Ep 20 | Batch 110 | Loss: 0.0971 | Acc: 100.00%\n",
      "Ep 20 | Batch 115 | Loss: 0.2174 | Acc: 87.50%\n",
      "Ep 20 | Batch 120 | Loss: 0.3428 | Acc: 84.38%\n",
      "Ep 20 | Batch 125 | Loss: 0.1783 | Acc: 90.62%\n",
      "Ep 20 | Batch 130 | Loss: 0.3213 | Acc: 90.62%\n",
      "Ep 20 | Batch 135 | Loss: 0.1925 | Acc: 96.88%\n",
      "Ep 20 | Batch 140 | Loss: 0.3535 | Acc: 84.38%\n",
      "===> End Ep 20 | Avg Loss: 0.2515 | Avg Acc: 91.78%\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd206a9",
   "metadata": {
    "papermill": {
     "duration": 0.02309,
     "end_time": "2026-01-01T10:13:26.326652",
     "exception": false,
     "start_time": "2026-01-01T10:13:26.303562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9001289,
     "sourceId": 14127388,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9131931,
     "sourceId": 14305282,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9165981,
     "sourceId": 14354608,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 285127185,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 285397454,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2150.063431,
   "end_time": "2026-01-01T10:13:27.769568",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-01T09:37:37.706137",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
